# Base configuration for PE comparison study
# Hyperparameters from DroPE paper (from-scratch training)

# Model settings
model:
  name_or_path: "HuggingFaceTB/SmolLM-360M"
  dtype: bfloat16
  from_scratch: true  # Initialize with random weights, not pretrained
  # Attention implementation:
  # - sdpa: PyTorch native, supports 4D attention masks for block diagonal
  # - flash_attention_2: fastest but requires different collator format
  # - eager: slowest, always works
  # Using sdpa for compatibility with block diagonal attention masks
  attn_implementation: "sdpa"

# Data settings
data:
  dataset_name: "HuggingFaceFW/fineweb"
  dataset_config: "sample-10BT"  # 10B token sample
  text_column: "text"
  streaming: true
  max_length: 1024  # DroPE paper training context

# Training settings (from DroPE paper: from-scratch training)
training:
  output_dir: "./outputs"
  max_steps: 16000  # S = 16k total steps
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 8  # effective batch size = 64
  learning_rate: 3.0e-4  # DroPE paper
  min_learning_rate: 3.0e-5
  weight_decay: 0.01
  max_grad_norm: 1.0
  warmup_steps: 480  # 3% of 16k steps
  lr_scheduler_type: "cosine"

  # Optimizer
  optim: "adamw_torch"
  adam_beta1: 0.9
  adam_beta2: 0.95

  # Precision
  bf16: true
  tf32: true

  # Gradient checkpointing - use non-reentrant mode to fix gradient computation
  gradient_checkpointing: true
  gradient_checkpointing_kwargs:
    use_reentrant: false

  # Checkpointing
  save_steps: 5000
  save_total_limit: 3

  # Logging
  logging_steps: 10
  report_to: "wandb"

  # Dataloader (must be 0 for streaming datasets with generators)
  dataloader_num_workers: 0

# W&B settings
wandb:
  project: "pe-comparison-study"
  entity: null  # Set to your wandb username/team
  tags: []

# Seed for reproducibility
seed: 42
